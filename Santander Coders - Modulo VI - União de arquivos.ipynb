{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "585b242c-a602-4d3c-afd1-c970e526ef53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#União arquivos particionados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91797234-8e5c-4313-82db-c54b7bcccdb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Os arquivos foram particionados no disco local e foi feito upload de forma manual via interface gráfica do Databricks. Provavelmente essa não era a abordagem mais eficaz de subir o arquivo original no ambiente, mas serviu como aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04cbed3d-ce17-48e7-b5af-d347dac866cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Abordagem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f12f3e-00e8-4227-80e4-6d92862c4564",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Essa não foi a primeira abordagem. A primeira de fato demorou muito e não avançou quase nada, por isso foi descartada.\n",
    "\n",
    "Como o primeiro código (que não foi apresentado) estava demorando demais, comecei a buscar outras formas de fazer a união e pensei em diminuir a quantidade de partições (por se tratar de um cluster) do arquivo para tentar ser mais rápido. De fato o resultado foi melhor, porém por um erro no particionamento feito no disco local tive que adaptar a leitura das partes e consequentemente a união.\n",
    "\n",
    "No disco local eu simplesmente quebrei o arquivo original em partes levando em conta o número de linhas, porém o cabeçalho estava presente apenas na parte 1. Isso gerou alguns problemas na união, que foram corrigidos na Abordagem 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cf99873-93c6-40db-8869-166a977b5313",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.dbutils\n",
    "import pyspark.pandas as ps\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "113cc9e7-b095-488e-b7df-e92e1cad0bf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"dbfs:/FileStore/\"\n",
    "\n",
    "files = dbutils.fs.ls(path)\n",
    "\n",
    "df_res = ps.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2737c71a-6daa-4c32-9a41-01ae7d45cdaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23511b6f-337c-465a-bad7-7fd36d5e30fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fim da leitura\nfim da Coalesced\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inicializa a sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"União de Arquivos\").getOrCreate()\n",
    "\n",
    "# Caminho do diretório onde os arquivos divididos estão localizados\n",
    "diretorio_entrada = \"dbfs:/FileStore/\"\n",
    "\n",
    "# Obtém a lista de arquivos no diretório de entrada\n",
    "arquivos = [arquivo.path for arquivo in dbutils.fs.ls(diretorio_entrada) if arquivo.path.endswith(\".csv\")]\n",
    "\n",
    "# Ordena a lista de arquivos com base no número da parte\n",
    "arquivos_ordenados = sorted(arquivos, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Lê cada arquivo CSV e cria um DataFrame Spark para cada um\n",
    "for arquivo in arquivos_ordenados:\n",
    "    df = spark.read.csv(arquivo, header=True, inferSchema=True, sep=';')\n",
    "    dataframes.append(df)\n",
    "\n",
    "print(\"fim da leitura\")\n",
    "\n",
    "# Reduz o número de partições antes de unir os DataFrames\n",
    "num_partitions = 10  # Defina um número adequado de partições\n",
    "dataframes_coalesced = [df.coalesce(num_partitions) for df in dataframes]\n",
    "\n",
    "print(\"fim da Coalesced\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46921cc4-19db-44bc-a1ec-b3d8217127df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1660463438321834>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Concatena os DataFrames em um único DataFrame\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m df_consolidado \u001B[38;5;241m=\u001B[39m reduce(\u001B[38;5;28;01mlambda\u001B[39;00m x, y: x\u001B[38;5;241m.\u001B[39munion(y), dataframes_coalesced)\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfim da lambda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Exibe o DataFrame consolidado\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#df_consolidado.show()\u001B[39;00m\n",
       "\u001B[1;32m      8\u001B[0m \n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Salva o DataFrame consolidado em formato CSV\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m<command-1660463438321834>:2\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x, y)\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Concatena os DataFrames em um único DataFrame\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m df_consolidado \u001B[38;5;241m=\u001B[39m reduce(\u001B[38;5;28;01mlambda\u001B[39;00m x, y: \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m, dataframes_coalesced)\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfim da lambda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Exibe o DataFrame consolidado\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#df_consolidado.show()\u001B[39;00m\n",
       "\u001B[1;32m      8\u001B[0m \n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Salva o DataFrame consolidado em formato CSV\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3646\u001B[0m, in \u001B[0;36mDataFrame.union\u001B[0;34m(self, other)\u001B[0m\n",
       "\u001B[1;32m   3598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munion\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   3599\u001B[0m     \u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n",
       "\u001B[1;32m   3600\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3601\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3644\u001B[0m \u001B[38;5;124;03m    +----+----+----+\u001B[39;00m\n",
       "\u001B[1;32m   3645\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3646\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 1 columns and the second input has 2 columns.;\n",
       "'Union false, false\n",
       ":- Repartition 10, false\n",
       ":  +- Relation [uf;municipio;codigo_ibge;area_do_imovel;registro_car;situacao_cadastro;condicao_cadastro;area_liquida;area_remanescente_vegetacao_nativa;area_reserva_legal_proposta;area_preservacao_permanente;area_nao_classificada;solicitacao_adesao_pra;latitude;longitude;data_inscricao;data_alteracao_condicao_cadastro;area_rural_consolidada;area_servidao_administrativa;tipo_imovel_rural;modulos_fiscais;area_uso_restrito;area_reserva_legal_averbada;area_reserva_legal_aprovada_nao_averbada;area_pousio;data_ultima_retificacao#9392346] csv\n",
       "+- Repartition 10, false\n",
       "   +- Relation [SC;Planalto Alegre;4213153;2.8657;SC-4213153-2813F395FDE246DE99FFAAFA95CFEA53;AT;Aguardando análise#9393808, não passível de revisão de dados;2.8657;0;0.0000;0.451884412152437;0.00881973484027077;Nao;-27.0223867016797;-52.8664047499872;2014-07-01 16:39:40.543;;2.77840902540383;0;IRU;0.1433;0;0.0000;0.0000;0;2014-07-01 16:39:40.543#9393809] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1660463438321834>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Concatena os DataFrames em um único DataFrame\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_consolidado \u001B[38;5;241m=\u001B[39m reduce(\u001B[38;5;28;01mlambda\u001B[39;00m x, y: x\u001B[38;5;241m.\u001B[39munion(y), dataframes_coalesced)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfim da lambda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Exibe o DataFrame consolidado\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#df_consolidado.show()\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Salva o DataFrame consolidado em formato CSV\u001B[39;00m\n\nFile \u001B[0;32m<command-1660463438321834>:2\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Concatena os DataFrames em um único DataFrame\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_consolidado \u001B[38;5;241m=\u001B[39m reduce(\u001B[38;5;28;01mlambda\u001B[39;00m x, y: \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m, dataframes_coalesced)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfim da lambda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Exibe o DataFrame consolidado\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#df_consolidado.show()\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Salva o DataFrame consolidado em formato CSV\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3646\u001B[0m, in \u001B[0;36mDataFrame.union\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   3598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munion\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3599\u001B[0m     \u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   3600\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3601\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3644\u001B[0m \u001B[38;5;124;03m    +----+----+----+\u001B[39;00m\n\u001B[1;32m   3645\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3646\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 1 columns and the second input has 2 columns.;\n'Union false, false\n:- Repartition 10, false\n:  +- Relation [uf;municipio;codigo_ibge;area_do_imovel;registro_car;situacao_cadastro;condicao_cadastro;area_liquida;area_remanescente_vegetacao_nativa;area_reserva_legal_proposta;area_preservacao_permanente;area_nao_classificada;solicitacao_adesao_pra;latitude;longitude;data_inscricao;data_alteracao_condicao_cadastro;area_rural_consolidada;area_servidao_administrativa;tipo_imovel_rural;modulos_fiscais;area_uso_restrito;area_reserva_legal_averbada;area_reserva_legal_aprovada_nao_averbada;area_pousio;data_ultima_retificacao#9392346] csv\n+- Repartition 10, false\n   +- Relation [SC;Planalto Alegre;4213153;2.8657;SC-4213153-2813F395FDE246DE99FFAAFA95CFEA53;AT;Aguardando análise#9393808, não passível de revisão de dados;2.8657;0;0.0000;0.451884412152437;0.00881973484027077;Nao;-27.0223867016797;-52.8664047499872;2014-07-01 16:39:40.543;;2.77840902540383;0;IRU;0.1433;0;0.0000;0.0000;0;2014-07-01 16:39:40.543#9393809] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 1 columns and the second input has 2 columns.;\n'Union false, false\n:- Repartition 10, false\n:  +- Relation [uf;municipio;codigo_ibge;area_do_imovel;registro_car;situacao_cadastro;condicao_cadastro;area_liquida;area_remanescente_vegetacao_nativa;area_reserva_legal_proposta;area_preservacao_permanente;area_nao_classificada;solicitacao_adesao_pra;latitude;longitude;data_inscricao;data_alteracao_condicao_cadastro;area_rural_consolidada;area_servidao_administrativa;tipo_imovel_rural;modulos_fiscais;area_uso_restrito;area_reserva_legal_averbada;area_reserva_legal_aprovada_nao_averbada;area_pousio;data_ultima_retificacao#9392346] csv\n+- Repartition 10, false\n   +- Relation [SC;Planalto Alegre;4213153;2.8657;SC-4213153-2813F395FDE246DE99FFAAFA95CFEA53;AT;Aguardando análise#9393808, não passível de revisão de dados;2.8657;0;0.0000;0.451884412152437;0.00881973484027077;Nao;-27.0223867016797;-52.8664047499872;2014-07-01 16:39:40.543;;2.77840902540383;0;IRU;0.1433;0;0.0000;0.0000;0;2014-07-01 16:39:40.543#9393809] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatena os DataFrames em um único DataFrame\n",
    "df_consolidado = reduce(lambda x, y: x.union(y), dataframes_coalesced)\n",
    "\n",
    "print(\"fim da lambda\")\n",
    "\n",
    "# Exibe o DataFrame consolidado\n",
    "#df_consolidado.show()\n",
    "\n",
    "# Salva o DataFrame consolidado em formato CSV\n",
    "caminho_saida_csv = \"dbfs:/dbfs/FileStore/tables/moduloVI/consolidado.csv\"\n",
    "df_consolidado.write.csv(caminho_saida_csv, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Salva o DataFrame consolidado em formato Parquet\n",
    "caminho_saida_parquet = \"dbfs:/dbfs/FileStore/tables/moduloVI/consolidado.parquet\"\n",
    "df_consolidado.write.parquet(caminho_saida_parquet, mode=\"overwrite\")\n",
    "\n",
    "# Encerra a sessão do Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d22b346-42f1-492b-8a07-13ea82ed67ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Abordagem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb63c874-f609-4eca-843f-a4725f764d56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nessa abordagem a grande diferença foi a leitura separada da primeira parte, que tinha o cabeçalho, para as outras partes, que não tinham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1447f4-51f8-4bfb-977e-7e34f0eeac53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "from pyspark.sql.types import StructType, StringType  # Importa os tipos de dados\n",
    "\n",
    "# Inicializa a sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"União de Arquivos\").getOrCreate()\n",
    "\n",
    "# Caminho do diretório onde os arquivos divididos estão localizados\n",
    "diretorio_entrada = \"dbfs:/FileStore/\"\n",
    "\n",
    "# Obtém a lista de arquivos no diretório de entrada\n",
    "arquivos = [arquivo.path for arquivo in dbutils.fs.ls(diretorio_entrada) if arquivo.path.endswith(\".csv\")]\n",
    "\n",
    "# Ordena a lista de arquivos com base no número da parte\n",
    "arquivos_ordenados = sorted(arquivos, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Lê o primeiro arquivo para obter o cabeçalho (esquema)\n",
    "primeiro_arquivo = arquivos_ordenados[0]\n",
    "primeiro_df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(primeiro_arquivo)\n",
    "esquema = primeiro_df.schema\n",
    "\n",
    "# Lê os outros arquivos usando o mesmo esquema do primeiro arquivo\n",
    "for arquivo in arquivos_ordenados:\n",
    "    if arquivo != primeiro_arquivo:\n",
    "        df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \";\").schema(esquema).csv(arquivo)\n",
    "        dataframes.append(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608d6637-5862-47d8-a21b-dd4040b4d453",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatena os DataFrames em um único DataFrame\n",
    "df_consolidado = reduce(lambda x, y: x.union(y), dataframes)\n",
    "\n",
    "# Salva o DataFrame consolidado em formato CSV\n",
    "caminho_saida_csv = \"dbfs:/dbfs/FileStore/tables/moduloVI/consolidado.csv\"  # Substitua pelo seu caminho de saída CSV\n",
    "df_consolidado.write.csv(caminho_saida_csv, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Salva o DataFrame consolidado em formato Parquet\n",
    "caminho_saida_parquet = \"dbfs:/dbfs/FileStore/tables/moduloVI/consolidado.parquet\"  # Substitua pelo seu caminho de saída Parquet\n",
    "df_consolidado.write.parquet(caminho_saida_parquet, mode=\"overwrite\")\n",
    "\n",
    "# Encerra a sessão do Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Santander Coders - Modulo VI - União de arquivos",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
